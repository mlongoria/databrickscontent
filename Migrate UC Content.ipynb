{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migration Databricks Unity Catalog data and schema from one catalog to another.  \n",
    "Created by Meagan Longoria  \n",
    "Last updated 25 Sep 2024  \n",
    "  \n",
    "Pre-reqs:   \n",
    "    Must have target external location and target catalog created before running the notebook  \n",
    "    Must have owner user/group created before running the notebook  \n",
    "Assumptions:   \n",
    "    For views and volumes that reference external storage, assumes there is only one source or target storage account  \n",
    "Notes:  \n",
    "    Notebook is for running interactively. It prints or displays results to help you understand progress  \n",
    "    Creates managed tables (and copies data), views, external volumes, managed volumes (does not copy data)  \n",
    "    Last process is to change the object owner for each object type (table, view, volume)  \n",
    "    Filters out objects in information schema, but includes objects in default schema  \n",
    "General pattern:  \n",
    "    1. Create a dataframe with the objects (tables/views/volumes) to be copied.  \n",
    "    2. Create a function that dynamically generates the SQL DDL and executes it.  \n",
    "    3. Loop through the dataframe, check if the object already exists, and call the function to create as needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bb553f-881d-46b3-b388-7302162666c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create input widgets (only run once)\n",
    "dbutils.widgets.text(\"source_catalog\", \"\")\n",
    "dbutils.widgets.text(\"target_catalog\", \"\")\n",
    "dbutils.widgets.text(\"source_storage\",\"\")\n",
    "dbutils.widgets.text(\"target_storage\",\"\")\n",
    "dbutils.widgets.text(\"assigned_owner\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ab2a13-5508-458e-a789-7faf424e77b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assign widget values to variables\n",
    "source_catalog = dbutils.widgets.get(\"source_catalog\")\n",
    "target_catalog = dbutils.widgets.get(\"target_catalog\")\n",
    "source_storage = dbutils.widgets.get(\"source_storage\")\n",
    "target_storage = dbutils.widgets.get(\"target_storage\")\n",
    "target_storage = dbutils.widgets.get(\"assigned_owner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9bf61f-1799-42ca-8204-70fcf0665e55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Copy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8552df-4398-44cc-95f4-36c8910b2a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get dataframe of managed tables\n",
    "df_tables = spark.sql(f\"Select * from {source_catalog}.information_schema.tables where table_type = 'MANAGED'\")\n",
    "display(df_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3211b586-28df-4c79-a35e-7638d2294285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function for copying tables\n",
    "def copy_table(source_catalog, target_catalog, schema, table):\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{schema};\")\n",
    "\n",
    "    if spark.catalog.tableExists(f\"{target_catalog}.{schema}.{table}\"):\n",
    "        print(f\"Skip table {target_catalog}.{schema}.{table}, because the table already exists.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Copy from {source_catalog}.{schema}.{table} into {target_catalog}.{schema}.{table}.\")\n",
    "        source_df = spark.table(f\"{source_catalog}.{schema}.{table}\")\n",
    "    \n",
    "        renamed_cols = [col.replace(\" \", \"_\") for col in source_df.columns]\n",
    "\n",
    "        renamed_df = source_df.toDF(*renamed_cols)\n",
    "\n",
    "        renamed_df.write.mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{schema}.{table}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e1ade5-9436-447d-9814-2e507cfc2b92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# loop to call the function for each table\n",
    "for index, row in df_tables.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    table = row[2]\n",
    "    copy_table(source_catalog, target_catalog, schema, table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6c34e2-65d4-423e-a5bf-f887245ee7e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check to see if there are any tables that did not get created in target\n",
    "df_tables_s = spark.sql(f\"Select table_schema, table_name from {source_catalog}.information_schema.tables where table_type = 'MANAGED'\")\n",
    "df_tables_t = spark.sql(f\"Select table_schema, table_name from {target_catalog}.information_schema.tables where table_type = 'MANAGED'\")\n",
    "df_tables_left = df_tables_s.exceptAll(df_tables_t)\n",
    "display(df_tables_left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f35ef20-11e1-415a-ba71-2c57f5188f48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Copy Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca698e8-737a-404c-92ae-81ee79bb9e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to copy views\n",
    "def migrate_view (source_catalog, target_catalog, schema, viewnm):\n",
    "        vwdef = spark.sql(f\"SHOW CREATE TABLE {source_catalog}.{schema}.{viewnm};\")\n",
    "        vwdefp = vwdef.toPandas()\n",
    "        defstr = vwdefp['createtab_stmt'][0]\n",
    "        #if view definition contain 3-part name that references catalog, replace source catalog with target catalog in definition\n",
    "        defstrdb = defstr.replace(source_catalog,target_catalog)\n",
    "        #if view definition contains reference to storage account, replace source storage with target storage account (make sure external location has been configured in target)\n",
    "        defstrstr = defstrdb.replace(source_storage, target_storage)\n",
    "        print (defstrstr)\n",
    "        spark.sql(f\"Use catalog {target_catalog};\")\n",
    "        spark.sql(f\"Use schema {schema};\")\n",
    "        spark.sql(defstrstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e008b70b-47f4-4a3c-a6e3-1fd659c73e02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get df of views\n",
    "df_views = spark.sql(f\"Select * from {source_catalog}.information_schema.views where table_schema <> 'information_schema'\")\n",
    "# loop through views\n",
    "for index, row in df_views.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    viewnm = row[2]\n",
    "# Check if the view exists \n",
    "    view_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.views \n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_name = '{viewnm}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if view_exists:\n",
    "        print(f\"Skip view {target_catalog}.{schema}.{viewnm}, because the view already exists.\")\n",
    "    else:\n",
    "        migrate_view(source_catalog, target_catalog, schema, viewnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585dd814-9d2b-4ca8-b621-ea695dd2f37a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check if all views were copied\n",
    "df_views_s = spark.sql(f\"Select table_schema, table_name from {source_catalog}.information_schema.views\")\n",
    "df_views_t = spark.sql(f\"Select table_schema, table_name from {target_catalog}.information_schema.views\")\n",
    "df_views_left = df_views_s.exceptAll(df_views_t)\n",
    "display(df_views_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e88614-f773-4787-9a5e-5886016c0e77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Copy volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa620e8a-5865-4c45-9205-1c002ee18e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to copy EXTERNAL volumnes\n",
    "def migrate_vol (source_catalog, target_catalog, schema, volnm):\n",
    "        voldef = spark.sql(f\"DESCRIBE VOLUME {source_catalog}.{schema}.{volnm};\")\n",
    "        voldefp = voldef.toPandas()\n",
    "        crstr = f\"CREATE EXTERNAL VOLUME {target_catalog}.{schema}.{volnm} LOCATION '\"\n",
    "        defstr = voldefp['storage_location'][0]\n",
    "        defstrstr = defstr.replace(source_storage, target_storage)\n",
    "        finalstr = crstr + defstrstr + \"';\";\n",
    "        print (finalstr)\n",
    "        spark.sql(finalstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fdc5065-a163-4b2e-89d0-656889aa423f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get df of external volumes\n",
    "df_vols = spark.sql(f\"Select * from {source_catalog}.information_schema.volumes WHERE volume_type = 'EXTERNAL'\")\n",
    "\n",
    "for index, row in df_vols.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    volnm = row[2]\n",
    "# Check if the volume exists \n",
    "    vol_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.volumes \n",
    "        WHERE volume_schema = '{schema}' \n",
    "        AND volume_name = '{volnm}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if vol_exists:\n",
    "        print(f\"Skip volume {target_catalog}.{schema}.{volnm}, because the volume already exists.\")\n",
    "    else:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{schema};\")\n",
    "        migrate_vol(source_catalog, target_catalog, schema, volnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6c026f-c5a1-4a55-96ee-490c23fd191a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to migrate MANAGED volumes\n",
    "def migrate_mvol (source_catalog, target_catalog, schema, volnm):\n",
    "        voldef = spark.sql(f\"DESCRIBE VOLUME {source_catalog}.{schema}.{volnm};\")\n",
    "        voldefp = voldef.toPandas()\n",
    "        crstr = f\"CREATE VOLUME {target_catalog}.{schema}.{volnm} \"\n",
    "        print (crstr)\n",
    "        spark.sql(finalstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61acd03a-0a62-4aa8-9b4a-5a5a33276983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get df of managed volumes\n",
    "df_vols = spark.sql(f\"Select * from {source_catalog}.information_schema.volumes WHERE volume_type = 'MANAGED'\")\n",
    "\n",
    "# loop through volumes in df\n",
    "for index, row in df_vols.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    volnm = row[2]\n",
    "\n",
    "# Check if the volume exists \n",
    "    vol_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.volumes \n",
    "        WHERE volume_schema = '{schema}' \n",
    "        AND volume_name = '{volnm}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if vol_exists:\n",
    "        print(f\"Skip volume {target_catalog}.{schema}.{volnm}, because the volume already exists.\")\n",
    "    else:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{schema};\")\n",
    "        migrate_mvol(source_catalog, target_catalog, schema, volnm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd08ab0-78b0-4580-9182-d358d878dcd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Change object owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e176dc-f9ef-48c5-9727-2789161be824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to change volume owner\n",
    "def migrate_volowner (schema, volnm):\n",
    "        crstr = f\"Alter VOLUME {target_catalog}.{schema}.{volnm} OWNER TO `{assigned_owner}`;\"\n",
    "        print (crstr)\n",
    "        spark.sql(crstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f6f5d8-3dc7-46dd-9724-0ffe6cf0c7dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get df of volumes that need owner changed\n",
    "df_vols = spark.sql(f\"Select * from {target_catalog}.information_schema.volumes WHERE volume_owner <> '{assigned_owner}'\")\n",
    "\n",
    "# loop through volumes\n",
    "for index, row in df_vols.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    volnm = row[2]\n",
    "\n",
    "# Check if the volume exists with correct owner\n",
    "    vol_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.volumes \n",
    "        WHERE volume_schema = '{schema}' \n",
    "        AND volume_name = '{volnm}'\n",
    "        AND volume_owner = '{assigned_owner}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if vol_exists:\n",
    "        print(f\"Skip volume {target_catalog}.{schema}.{volnm}, because the volume owner already changed.\")\n",
    "    else:\n",
    "        migrate_volowner(schema, volnm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b258c42-c42f-4363-951b-616d599f2916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to update table owner\n",
    "def migrate_tableowner (schema, tablnm):\n",
    "        crstr = f\"Alter Table {target_catalog}.{schema}.{tablnm} OWNER TO `{assigned_owner}`;\"\n",
    "        print (crstr)\n",
    "        spark.sql(crstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c371200-74b6-4be9-81c1-d00758e9bbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get df of tables that need owner change\n",
    "df_tbl = spark.sql(f\"Select * from {target_catalog}.information_schema.tables WHERE table_owner <> '{assigned_owner}' and table_schema not in ('information_schema')\")\n",
    "\n",
    "# loop through df of tables\n",
    "for index, row in df_tbl.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "    tblnm = row[2]\n",
    "# Check if the table exists with correct owner\n",
    "    vol_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.tables\n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_name = '{tblnm}'\n",
    "        AND table_owner = '{assigned_owner}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if vol_exists:\n",
    "        print(f\"Skip table/view {target_catalog}.{schema}.{tblnm}, because the owner already changed.\")\n",
    "    else:\n",
    "        migrate_tableowner(schema, tblnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eda28ea-d1a1-47b7-b1e5-5185d9410b05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to change schema owner\n",
    "def migrate_schemaowner (schema):\n",
    "        crstr = f\"\"\"Alter schema {target_catalog}.{schema} OWNER TO `{assigned_owner}`;\"\"\"\n",
    "        print (crstr)\n",
    "        spark.sql(crstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191e48c4-01f1-4b9e-93ce-5f33cad537e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get list of schemas that need owner changed\n",
    "df_schema = spark.sql(f\"Select * from {target_catalog}.information_schema.schemata where schema_name not in ('information_schema') and schema_owner <> '{assigned_owner}' \")\n",
    "\n",
    "# loop through df of schemas\n",
    "for index, row in df_schema.toPandas().iterrows():\n",
    "    schema = row[1]\n",
    "  \n",
    "# Check if the schema exists with correct owner\n",
    "    vol_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM {target_catalog}.information_schema.schemata\n",
    "        WHERE schema_name = '{schema}' \n",
    "        AND schema_owner = '{assigned_owner}'\n",
    "    \"\"\").collect()[0][0] > 0\n",
    "    \n",
    "    if vol_exists:\n",
    "        print(f\"Skip schema {target_catalog}.{schema}, because the owner already changed.\")\n",
    "    else:\n",
    "        migrate_schemaowner(schema)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3352461618683372,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Migrate UC Content",
   "widgets": {
    "source_catalog": {
     "currentValue": "examstest",
     "nuid": "616caf98-fec7-472e-9fec-e19c9b69fbc2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "source_catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "source_catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "target_catalog": {
     "currentValue": "examsprod",
     "nuid": "d4865fc8-2213-470d-8d10-11811e3b0894",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "target_catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
